{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports and Downloads  \n",
    "### NLTK library is used as the main resource for the NLP algorithm \n",
    "### The following inputs are used to:  \n",
    "    1. 'twitter_samples': create the model which is based out of tweets so samples may be downloaded for the dataset  \n",
    "    2. 'punkt': module which is pre-trained to tokenize words and sentences  \n",
    "    3. 'wordnet': lexical database for the English language  \n",
    "    4. 'averaged_perceptron_tagger': resource to determine the context of a word in a sentence  \n",
    "    5. 'stopwords': common stopwords in English language"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# import NLTK library which is the Natural Language Toolkit adn other necessary sub-librairies\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "import re, string\n",
    "import random"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/alexandreberkovic/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/alexandreberkovic/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/alexandreberkovic/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/alexandreberkovic/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/alexandreberkovic/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "source": [
    "# Pre-processing and data cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the positive and negative tweets to create the dataset (5k points each)\n",
    "# the text variable is a dataset of 20k points with no sentiments\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# pos_tag is a tagging algorithm which assesses the relative position of a word in a sentence\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    '''\n",
    "    Lemmatization normalises a word with the context of vocabulary\n",
    "    and mophological anlysis of words in text. It analyses the structure of the word and its context to normalise it.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    '''\n",
    "    Removes noise from the dataset such as hyperlinks, punctuation, special characters and incroporatrs the normalisation and lemmatization mentioned previously.\n",
    "    '''\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(remove_noise(tweet_tokens[0], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizes the data by sequencing the characters in the text as individual units and enables to process natural language\n",
    "positive_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    '''\n",
    "    generator function that takes a list of corpuses as arguments to provide a list of tokens in all of the corpuses.\n",
    "    '''\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "# enables to display the most common words\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    '''\n",
    "    generator function that changes the format of the cleaned data to a Python dictionnary so that it may be fed to the model\n",
    "    '''\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "source": [
    "# Dataset Creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# creates a positive dataset\n",
    "for token in positive_tokens_for_model:\n",
    "    positive_df.append((token, \"Positive\"))"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a negative dataset\n",
    "for token in negative_tokens_for_model:\n",
    "    negative_df.append((token, \"Negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines the datasets, shuffles them and splits them into a training set and a test set\n",
    "df = list(positive_df) + negative_df\n",
    "random.shuffle(df)\n",
    "\n",
    "train_df = df[:7000]\n",
    "test_df = df[7000:]"
   ]
  },
  {
   "source": [
    "# Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy is: 0.49838461538461537\nMost Informative Features\n           #FollowFriday = True           Negati : Positi =      1.0 : 1.0\n                      :) = True           Negati : Positi =      1.0 : 1.0\n            @France_Inte = True           Negati : Positi =      1.0 : 1.0\n          @Milipol_Paris = True           Negati : Positi =      1.0 : 1.0\n              @PKuchly57 = True           Negati : Positi =      1.0 : 1.0\n                   being = True           Negati : Positi =      1.0 : 1.0\n               community = True           Negati : Positi =      1.0 : 1.0\n                 engaged = True           Negati : Positi =      1.0 : 1.0\n                     for = True           Negati : Positi =      1.0 : 1.0\n                      in = True           Negati : Positi =      1.0 : 1.0\nNone\n"
     ]
    }
   ],
   "source": [
    "# we use a simple Naive Bayes Classifier to buid the model as it displays high accuracies\n",
    "classifier = NaiveBayesClassifier.train(train_df)\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_df))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# create a custom token to test the model\n",
    "custom_sentence = \"what a beautiful day i feel great\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_sentence))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "source": [
    "# Flagging\n",
    "After conversations with mental health care nurses, we quickly understood that key sentences needed to be flagged for immediate action to be taken by carers or relatives. Below is that implementation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['i feel depressed', 'i cannot do it anymore', '...']\n",
    "text = \"i feel depressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning(text, sentences):\n",
    "    '''\n",
    "    Flags sentences which should directly warn carers or relatives \n",
    "    as they display extreme swings of emotion which can be harmful\n",
    "    '''\n",
    "    for sentence in sentences:\n",
    "        if sentence == text:\n",
    "            return 'WARNING'\n",
    "            break\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'WARNING'"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "warning(text, sentences)"
   ]
  }
 ]
}