{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection Model generation\n",
    "\n",
    "Our Futures project's goal is to create an empathetic, predicitve system for people with dementia. Hence, we want to use emotional data to help support our predicitve system which is why prototyping an emotion recognition algorithm was essential. For this project a Convolutional Neural Network is built from scratch by using Keras. To understand this thouroughly you should have some basic knowledge of python, Convolutional Neural Network(CNN) and the different layers which are used in CNN. The notebook is divided into tasks to make it easier to understand.\n",
    "\n",
    "## Task 1:\n",
    "\n",
    "Import the required modules that are needed in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some variables that will save the time of writing the values manually again and again.\n",
    "\n",
    "- num_classes:  defines the number of outputs (emotions) that the algorithm can give\n",
    "- img_rows,img_cols: defines the size of the array fed to the NN (image size)\n",
    "- batch_size: defines the batch size which is the number of samples processed before the model is updated. The number of epochs is the number of complete passes through the training dataset. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=5 \n",
    "img_rows,img_cols=48,48 \n",
    "batch_size=32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "Now it's time to load our dataset (**fer2013**) dataset which is  open source and hosted on [kaggle](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data). The data is segragated as training and validation set.\n",
    "\n",
    "The lines below import the validation and training data from local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir='fer2013/train'\n",
    "validation_data_dir='fer2013/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "\n",
    "Image Augmentation techniques are used on the dataset. This is a technique that can be used for artificial expansion of the size of a training dataset by creating modified versions of images. Keras provides the capability to fit models using image data augmentation via the ImageDataGenerator class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_datagen variable expands the dataset using the following:\n",
    "\n",
    "- rotation_range: degree range for random rotations (30$^\\circ$)\n",
    "- shear_range: shear intensity (counter-clockwise direction)\n",
    "- zoom_range: range for random zoom (0.3)\n",
    "- width_shift_range: shifts the images by a value across its width\n",
    "- height_shift_range : shifts the images by a value across its height\n",
    "- horizontal_flip: flips the images horizontally\n",
    "- fill_mode: used to fill in the pixels after making changes to the orientation of the images by the above used methods. 'nearest' is used as the fill mode instructing it to fill the missing pixels in the image with the nearby pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flow_from_directory()** method is used to load the dataset from the directory which is augmented and stored in the train_generator and validation_generator variables.**flow_from_directory()**  takes the path to a directory & generates batches of augmented data. \n",
    "\n",
    "The options given are:\n",
    "\n",
    "- directory:  directory of the dataset.\n",
    "- color_mode:  converting the images to grayscale as I am not interested in the color of the images but only the expressions.\n",
    "- target_size: converting the images to a uniform size.\n",
    "- batch_size:  make baches of data to train.\n",
    "- class_mode: 'categorical' is used as the class mode because the images are assigned to 5 classes.\n",
    "- shuffle: shuffles the dataset for better training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 24256 images belonging to 5 classes.\n",
      "Found 3006 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        train_data_dir,\n",
    "                        color_mode='grayscale',\n",
    "                        target_size=(img_rows,img_cols),\n",
    "                        batch_size=batch_size,\n",
    "                        class_mode='categorical',\n",
    "                        shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                                validation_data_dir,\n",
    "                                color_mode='grayscale',\n",
    "                                target_size=(img_rows,img_cols),\n",
    "                                batch_size=batch_size,\n",
    "                                class_mode='categorical',\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "\n",
    "Now that the dataset pre-processing is complete, we can construct the CNN to create this DL model. A **Sequential** model is used as it  defines that all the layers in the network will be one after the other sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "source": [
    "The network consists of 7 blocks which will be explained below.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i have used the layers of 7 types which are present in **keras.layers**.\n",
    "\n",
    "The layers are:\n",
    "\n",
    "- Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "- Activation(activation_type)\n",
    "\n",
    "- BatchNormalization()\n",
    "\n",
    "- MaxPooling2D(pool_size, strides, padding, data_format, **kwargs)\n",
    "\n",
    "- Dropout(dropout_value) \n",
    "\n",
    "- Flatten()\n",
    "\n",
    "- Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs)\n",
    "\n",
    "#### Block-1 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Conv2D layer-  creates a convolutional layer for the network. The layer is created with 32 filters and a filter size of (3,3) with padding='same' to pad the image and using the kernel initializer he_normal. I have added 2 convolutional layers each followed by an activation and batch normalization layers.\n",
    "- Activation layer - elu activation is being used\n",
    "- BatchNormalization - normalize the activations of the previous layer at each batch (applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1)\n",
    "- MaxPooling2D layer - downsamples the input representation by taking the maximum value over the window defined by pool_size for each dimension along the features axis (pool_size of (2,2))\n",
    "- Dropout: technique where randomly selected neurons are ignored during training. A dropout of 0.5 is used which means that it will ignore half of the neurons.\n",
    "\n",
    "#### Block-2 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Same layers as block-1 but the convolutional layers have 64 filters.\n",
    "\n",
    "#### Block-3 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Same layers as block-1 but the convolutional layers have 128 filters.\n",
    "\n",
    "#### Block-4 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Same layers as block-1 but the convolutional layers have 256 filters.\n",
    "\n",
    "#### Block-5 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Flatten layer - flattens the output of the previous layers in a fallout layer or in other words in the form of a vector.\n",
    "- Dense layer - densely connected layer where each neuron is connected to every other neuron (64 neurons with a kernal initializer - he_normal)\n",
    "- These layers are followed by activation layer with elu activation , batch normalization and finally a dropout with 50% dropout.\n",
    "\n",
    "#### Block-6 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Same layers as block 5 but without flatten layer as the input for this block is already flattened.\n",
    "\n",
    "#### Block-7 layers in the order of occurrence are as follows :\n",
    "\n",
    "- Dense layer - Finally in the final block of the network, num_classes is used to create a dense layer having the same number of units as there are classes with a he_normal initializer.\n",
    "\n",
    "- Activation layer - using a softmax layer which is used for multi-class classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Block-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the overall structure of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_9 (Conv2D)            (None, 48, 48, 32)        320       \n_________________________________________________________________\nactivation_12 (Activation)   (None, 48, 48, 32)        0         \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 48, 48, 32)        128       \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 48, 48, 32)        9248      \n_________________________________________________________________\nactivation_13 (Activation)   (None, 48, 48, 32)        0         \n_________________________________________________________________\nbatch_normalization_12 (Batc (None, 48, 48, 32)        128       \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 24, 24, 32)        0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 24, 24, 32)        0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 24, 24, 64)        18496     \n_________________________________________________________________\nactivation_14 (Activation)   (None, 24, 24, 64)        0         \n_________________________________________________________________\nbatch_normalization_13 (Batc (None, 24, 24, 64)        256       \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 24, 24, 64)        36928     \n_________________________________________________________________\nactivation_15 (Activation)   (None, 24, 24, 64)        0         \n_________________________________________________________________\nbatch_normalization_14 (Batc (None, 24, 24, 64)        256       \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 12, 12, 64)        0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 12, 12, 64)        0         \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 12, 12, 128)       73856     \n_________________________________________________________________\nactivation_16 (Activation)   (None, 12, 12, 128)       0         \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 12, 12, 128)       512       \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 12, 12, 128)       147584    \n_________________________________________________________________\nactivation_17 (Activation)   (None, 12, 12, 128)       0         \n_________________________________________________________________\nbatch_normalization_16 (Batc (None, 12, 12, 128)       512       \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 6, 6, 128)         0         \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 6, 6, 128)         0         \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 6, 6, 256)         295168    \n_________________________________________________________________\nactivation_18 (Activation)   (None, 6, 6, 256)         0         \n_________________________________________________________________\nbatch_normalization_17 (Batc (None, 6, 6, 256)         1024      \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 6, 6, 256)         590080    \n_________________________________________________________________\nactivation_19 (Activation)   (None, 6, 6, 256)         0         \n_________________________________________________________________\nbatch_normalization_18 (Batc (None, 6, 6, 256)         1024      \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 3, 3, 256)         0         \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 3, 3, 256)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 2304)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                147520    \n_________________________________________________________________\nactivation_20 (Activation)   (None, 64)                0         \n_________________________________________________________________\nbatch_normalization_19 (Batc (None, 64)                256       \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nactivation_21 (Activation)   (None, 64)                0         \n_________________________________________________________________\nbatch_normalization_20 (Batc (None, 64)                256       \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 5)                 325       \n_________________________________________________________________\nactivation_22 (Activation)   (None, 5)                 0         \n=================================================================\nTotal params: 1,328,037\nTrainable params: 1,325,861\nNon-trainable params: 2,176\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a big network which consits of 1,328,037 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5:\n",
    "\n",
    "Now the only thing left is to compile and train the model. But first let's import some more things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the magic.\n",
    "\n",
    "Before compiling 3 callbacks are created to modify the code while it is compiling if needed. This is done using **keras.callbacks** class:\n",
    "\n",
    "#### Checkpoint( Function - ModelCheckpoint() )\n",
    "\n",
    "It will monitor the validation loss and will try to minimise the loss using the mode='min' property. When the checkpoint is reached it will save the best trained weights. Verbose=1 is just for visulaisation when the code created checkpoint. The following parametrs are being used:\n",
    "\n",
    "- filepath: path to save the model file (model file with the name EmotionDetectionModel.h5)\n",
    "- monitor: quantity to monitor (validation loss)\n",
    "- mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity.\n",
    "- save_best_only: if save_best_only=True, the latest best model according to the quantity monitored will not be overwritten.\n",
    "- verbose: int. 0: quiet, 1: update messages.\n",
    "\n",
    "#### Early Stopping ( Function - EarlyStopping() )\n",
    "\n",
    "This will stop the execution early by checking the following properties.\n",
    "\n",
    "- monitor:  quantity to monitor (validation loss).\n",
    "- min_delta: Minimum change in the monitored quantity to qualify as an improvement, absolute change of less than min_delta, will count as no improvement (assigned a value of 0)\n",
    "- patience: number of epochs with no improvement after which training will be stopped (value of 3)\n",
    "- restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used (assigned True)\n",
    "- verbose: int. 0: quiet, 1: update messages.\n",
    "\n",
    "#### Reduce Learning Rate ( Function - ReduceLROnPlateau() )\n",
    "\n",
    "Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced. The following properties are used:\n",
    "\n",
    "- monitor: to monitor a particular loss (validation loss here)\n",
    "- factor: factor by which the learning rate will be reduced. **new_lr = lr * factor**. (using 0.2 as factor).\n",
    "- patience: Number of epochs with no improvement after which learning rate will be reduced (initial value of 3)\n",
    "- min_delta: threshold for measuring the new optimum, to only focus on significant changes.\n",
    "- verbose: int. 0: quiet, 1: update messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to finally compile the model using **model.compile()** and fit or train the model on the dataset using **model.fit_generator()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model.compile()\n",
    "\n",
    "It has the following arguments:\n",
    "\n",
    "- loss: will determine the type of loss function to use in your code. We have categorical data in 5 categories or classes so 'categorical_crossentropy' loss is used.\n",
    "- optimizer: will determine the type of optimizer function to use in your code. Adam optimizer with learning rate 0.001 is used as it is the best optimizer for categorical data.\n",
    "- metrics: list of metrics to be evaluated by the model during training and testing. Here, accuracy is used as metric which will compile mu model according to the accuracy.\n",
    "\n",
    "#### model.fit_generator()\n",
    "\n",
    "Fits the model on data yielded batch-by-batch by a Python generator.\n",
    "\n",
    "It has the following arguments:\n",
    "\n",
    "- generator: The train_generator object that we created earlier.\n",
    "- steps_per_epochs: The steps to take on the training data in one epoch.\n",
    "- epochs: The total number of epochs (pass though the whole dataset once).\n",
    "- callbacks: The list containing all the callbacks that we created earlier.\n",
    "- validation_data: The validation_generator object that we created earlier.\n",
    "- validation_steps: The steps to take on the validation data in one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "755/755 [==============================] - 266s 353ms/step - loss: 1.8671 - accuracy: 0.2326 - val_loss: 1.6492 - val_accuracy: 0.2991\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64916, saving model to EmotionDetectionModel.h5\n",
      "Epoch 2/25\n",
      "755/755 [==============================] - 275s 364ms/step - loss: 1.5839 - accuracy: 0.2759 - val_loss: 1.5460 - val_accuracy: 0.2946\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64916 to 1.54597, saving model to EmotionDetectionModel.h5\n",
      "Epoch 3/25\n",
      "755/755 [==============================] - 537s 711ms/step - loss: 1.5494 - accuracy: 0.3025 - val_loss: 1.5584 - val_accuracy: 0.3067\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.54597\n",
      "Epoch 4/25\n",
      "755/755 [==============================] - 377s 499ms/step - loss: 1.5322 - accuracy: 0.3136 - val_loss: 1.4969 - val_accuracy: 0.3564\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.54597 to 1.49691, saving model to EmotionDetectionModel.h5\n",
      "Epoch 5/25\n",
      "755/755 [==============================] - 214s 284ms/step - loss: 1.4747 - accuracy: 0.3481 - val_loss: 1.4621 - val_accuracy: 0.4183\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.49691 to 1.46212, saving model to EmotionDetectionModel.h5\n",
      "Epoch 6/25\n",
      "755/755 [==============================] - 211s 280ms/step - loss: 1.3946 - accuracy: 0.4035 - val_loss: 1.1100 - val_accuracy: 0.4835\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.46212 to 1.10999, saving model to EmotionDetectionModel.h5\n",
      "Epoch 7/25\n",
      "755/755 [==============================] - 2149s 3s/step - loss: 1.3230 - accuracy: 0.4451 - val_loss: 1.4205 - val_accuracy: 0.5124\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.10999\n",
      "Epoch 8/25\n",
      "755/755 [==============================] - 214s 283ms/step - loss: 1.2631 - accuracy: 0.4779 - val_loss: 1.3554 - val_accuracy: 0.5266\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.10999\n",
      "Epoch 9/25\n",
      "755/755 [==============================] - 219s 291ms/step - loss: 1.2042 - accuracy: 0.5071 - val_loss: 0.9984 - val_accuracy: 0.4919\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.10999 to 0.99842, saving model to EmotionDetectionModel.h5\n",
      "Epoch 10/25\n",
      "755/755 [==============================] - 230s 305ms/step - loss: 1.1686 - accuracy: 0.5279 - val_loss: 1.3896 - val_accuracy: 0.5525\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.99842\n",
      "Epoch 11/25\n",
      "755/755 [==============================] - 243s 322ms/step - loss: 1.1405 - accuracy: 0.5427 - val_loss: 1.3137 - val_accuracy: 0.5575\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.99842\n",
      "Epoch 12/25\n",
      "755/755 [==============================] - 275s 364ms/step - loss: 1.1180 - accuracy: 0.5524 - val_loss: 0.7741 - val_accuracy: 0.5578\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.99842 to 0.77409, saving model to EmotionDetectionModel.h5\n",
      "Epoch 13/25\n",
      "755/755 [==============================] - 252s 333ms/step - loss: 1.1007 - accuracy: 0.5582 - val_loss: 1.3534 - val_accuracy: 0.5763\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77409\n",
      "Epoch 14/25\n",
      "755/755 [==============================] - 245s 325ms/step - loss: 1.0810 - accuracy: 0.5707 - val_loss: 1.7290 - val_accuracy: 0.5800\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77409\n",
      "Epoch 15/25\n",
      "755/755 [==============================] - 221s 293ms/step - loss: 1.0689 - accuracy: 0.5796 - val_loss: 0.9903 - val_accuracy: 0.5884\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77409\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 24176\n",
    "nb_validation_samples = 3006\n",
    "epochs=25\n",
    "\n",
    "history=model.fit_generator(\n",
    "                train_generator,\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                validation_data=validation_generator,\n",
    "                validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}